{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDI project summary.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Nx6Ia5G8E8b7",
        "MyuUq7Jj-ylv",
        "WHCf1odY-1-2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MDI Project summary\n",
        "\n",
        "The [EVS 2017 integrated dataset (ZA7500)](https://europeanvaluesstudy.eu/methodology-data-documentation/survey-2017/full-release-evs2017/) is available as .sav (SPSS) and .dta (Stata) files under [DOI 10.4232/1.13560](https://doi.org/10.4232/1.13560). \n",
        "\n",
        "According to the authors, it contains data from the following 34 countries:\n",
        "\n",
        "\"Albania (AL); Armenia (AM); Austria (AT); Azerbaijan (AZ); Bosnia and Herzegovina (BA); Bulgaria (BG); Belarus (BY); Switzerland (CH); Czechia (CZ); Germany (DE); Denmark (DK); Estonia (EE); Spain (ES); Finland (FI); France (FR); Great Britain (GB); Georgia (GE); Croatia (HR); Hungary (HU); Iceland (IS); Italy (IT); Lithuania (LT); Montenegro (ME); Netherlands (NL); North Macedonia (MK); Norway (NO); Poland (PL); Portugal (PT); Romania (RO); Serbia (RS); Russia (RU); Sweden (SE); Slovenia (SI); Slovakia (SK)\".\n",
        "\n",
        "We first tried converting the SPSS file to .csv in SPSS 28. \n",
        "This would result in system crashes on a computer with 16 GB RAM.\n",
        "Using the open source alternative [GNU PSPP](http://www.gnu.org/software/pspp/), we ran into the same issue.\n",
        "\n",
        "Because of these issues, we settled on using Stata SE 16.1. \n",
        "In Stata, we opened the dataset `ZA7500-v4.0.0.dta` which we patched using `ZA7500-v4.0.0.Stata_PATCH_1.zip`. The patched dataset was then exported to a .csv file.\n",
        "\n",
        "The notebook starts with importing this complete EVS dataset file. \n"
      ],
      "metadata": {
        "id": "cP_kOBd34OBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "1XbTh46J8RD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ1oQjf136J6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.spatial import distance_matrix, distance\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import string\n",
        "import graphviz\n",
        "\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "import json\n",
        "\n",
        "import time\n",
        "from geopy.distance import great_circle\n",
        "from shapely.geometry import MultiPoint\n",
        "from datetime import datetime as dt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering out irrelevant variables and reencoding answers into numbers"
      ],
      "metadata": {
        "id": "8hQAZ9lB_e2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 500 MB upload with 30 Mbit/s, this takes around 2-3 minutes to run\n",
        "data = pd.read_csv('https://cloud.hollander.online/s/Z8D9HbWJWJMP2Hf/download')"
      ],
      "metadata": {
        "id": "_AM3ESgT7VBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the column names + amount of columns\n",
        "count_columns = 0\n",
        "for col_name in data.columns: \n",
        "    print(col_name)\n",
        "    count_columns = count_columns + 1\n"
      ],
      "metadata": {
        "id": "RIT7DkvC72y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_columns)"
      ],
      "metadata": {
        "id": "1GaQwGA7_UYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the [Survey published in Great Britain](https://dbk.gesis.org/dbksearch/download.asp?id=66252), also available through [DOI 10.4232/1.13560](https://doi.org/10.4232/1.13560), we decide that variables 'v6', 'v9', 'v36', 'v51', 'v52', 'v53', 'v54', 'v55', 'v56', 'v57', 'v58', 'v59', 'v60', 'v61', 'v62', 'v63', 'v64', 'v93', 'v115', 'v134' and 'v196' are relevant to our research."
      ],
      "metadata": {
        "id": "lHi_gahw8eDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering out the relevant variables from the original dataset\n",
        "data_filtered = data.filter(['v6', 'v9', 'v36', 'v51', 'v52', 'v53', 'v54', 'v55', 'v56', 'v57', 'v58', 'v59', 'v60', 'v61', 'v62', 'v63', 'v64', 'v93', 'v115', 'v134', 'v196'])"
      ],
      "metadata": {
        "id": "nzr4-rbFlSYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the remaining number of columns\n",
        "count = 0\n",
        "for col_name in data_filtered.columns: \n",
        "    count = count + 1\n",
        "print(count)"
      ],
      "metadata": {
        "id": "lEXAVEaJ9bKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the data subset we just generated\n",
        "data_filtered"
      ],
      "metadata": {
        "id": "iOivFC9I9bqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reencoding the textual answers into numbers\n",
        "\n",
        "We used `data_filtered[\"v6\"].unique` to determine the answer options for each individual variable.\n",
        "Based on these options, we reencoded answers as follows."
      ],
      "metadata": {
        "id": "_ySawoPZQpSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_filtered[\"v6\"].replace({'not at all important': 1, 'not important': 2, 'quite important': 3, 'very important': 4, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v36\"].replace({'trust completely': 1, 'trust somewhat': 2, 'do not trust very much': 3, 'do not trust at all': 4, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v54\"].replace({'more than once week': 1, 'once a week': 2, \"once a month\": 3, \"only on specific holy days\": 4, \"once a year\": 5, \"less often\": 6, \"never, practically never\": 7, 'dont know':8, 'no answer':9, 'multiple answers Mail': 101}, inplace=True)\n",
        "data_filtered[\"v55\"].replace({'more than once week': 1, 'once a week': 2, \"once a month\": 3, \"only on specific holy days\": 4, \"once a year\": 5, \"less often\": 6, \"never, practically never\": 7, 'dont know':8, 'no answer':9, 'multiple answers Mail': 101}, inplace=True)\n",
        "data_filtered[\"v56\"].replace({'a religious person': 1, 'not a religious person': 2, \"a convinced atheist\": 3, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v63\"].replace({'not at all important': 1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, \"very important\": 10, 'dont know':88, 'no answer':99}, inplace=True)\n",
        "data_filtered[\"v9\"].replace({'mentioned': 1, 'not mentioned': 2, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v51\"].replace({'yes': 1, 'no': 2, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v52\"].replace({'Muslim': 1, 'Orthodox': 2, 'not applicable': 77, 'Roman catholic': 3,\n",
        "       'Protestant': 4, 'Other': 5, 'no answer': 99, 'dont know': 88, 'Jew': 6, 'Buddhist': 7,\n",
        "       'Free church/Non-conformist/Evangelical': 8, 'Hindu': 9}, inplace=True)\n",
        "data_filtered[\"v53\"].replace({'yes': 1, 'no': 2, 'dont know':8, 'no answer':9, 'not applicable': 7}, inplace=True)\n",
        "data_filtered[\"v57\"].replace({'yes': 1, 'no': 2, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v58\"].replace({'yes': 1, 'no': 2, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v59\"].replace({'yes': 1, 'no': 2, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v60\"].replace({'yes': 1, 'no': 2, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v61\"].replace({'yes': 1, 'no': 2, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v62\"].replace({'personal God': 1, 'spirit or life force': 2, \"don't know what to think\": 3, \"no spirit, God or life force\": 4, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v93\"].replace({'not mentioned': 2, 'mentioned': 1, 'no answer': 9, 'dont know': 8}, inplace=True)\n",
        "data_filtered[\"v64\"].replace({'every day': 1, 'once a week': 3, 'never': 7, 'several times a year': 5, 'at least once a month': 4, 'more than once week': 2, 'less often': 6,'no answer': 9, 'dont know': 8, 'multiple answers Mail': 101, 'other missing': 100}, inplace=True)\n",
        "data_filtered[\"v115\"].replace({'a great deal': 1, 'quite a lot': 2, 'not very much': 3, 'none at all': 4, 'dont know':8, 'no answer':9}, inplace=True)\n",
        "data_filtered[\"v134\"].replace({'not at all an essential characteristic of democracy': 1, 'dont know': 88, 'no answer': 99, 'an essential characteristic of democracy': 10 ,'it is against democracy [DO NOT READ OUT]': 0, 'item not included': 100, 'multiple answers Mail': 101}, inplace=True)\n",
        "data_filtered[\"v196\"].replace({'not at all important': 1, 'not important': 2, 'quite important': 3, 'very important': 4, 'dont know':8, 'no answer':9}, inplace=True)\n"
      ],
      "metadata": {
        "id": "bMWe0wUp9mUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning the dataset by dropping unusable responses\n",
        "\n",
        "Rows containing the following variables were dropped\n",
        "\n",
        "- 8: \"dont know\"\n",
        "- 88: \"dont know\"\n",
        "- 9: \"no answer\"\n",
        "- 99: \"no answer\"\n",
        "- 100: \"item not included\"\n",
        "- 101: \"multiple answers Mail\""
      ],
      "metadata": {
        "id": "L7CTwWGZ_o_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaning = data_filtered"
      ],
      "metadata": {
        "id": "NyNe3EMI-_nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaning = data_cleaning[data_cleaning.v6 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v6 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v9 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v9 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v36 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v36 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v51 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v51 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v52 != 88]\n",
        "data_cleaning = data_cleaning[data_cleaning.v52 != 99]\n",
        "data_cleaning = data_cleaning[data_cleaning.v53 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v53 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v54 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v54 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v54 != 101]\n",
        "\n",
        "data_cleaning = data_cleaning[data_cleaning.v55 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v55 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v56 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v56 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v57 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v57 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v58 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v58 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v59 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v59 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v60 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v60 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v61 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v61 != 9]\n",
        "\n",
        "data_cleaning = data_cleaning[data_cleaning.v62 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v62 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v63 != 88]\n",
        "data_cleaning = data_cleaning[data_cleaning.v63 != 99]\n",
        "data_cleaning = data_cleaning[data_cleaning.v64 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v64 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v64 != 100]\n",
        "data_cleaning = data_cleaning[data_cleaning.v93 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v93 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v115 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v115 != 9]\n",
        "data_cleaning = data_cleaning[data_cleaning.v134 != 88]\n",
        "data_cleaning = data_cleaning[data_cleaning.v134 != 99]\n",
        "data_cleaning = data_cleaning[data_cleaning.v134 != 100]\n",
        "data_cleaning = data_cleaning[data_cleaning.v196 != 8]\n",
        "data_cleaning = data_cleaning[data_cleaning.v196 != 9]"
      ],
      "metadata": {
        "id": "DETRCz-E-sX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing the cleanup, we verify our work by checking which unique answers are left."
      ],
      "metadata": {
        "id": "B77fwfn1B1uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "varlist = []\n",
        "for col_name in data_cleaning.columns: \n",
        "    varlist.append(col_name)\n",
        "for var in varlist:\n",
        "  print(var, data_cleaning[var].unique())"
      ],
      "metadata": {
        "id": "2jaIJ4koBz0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalising the answers \n",
        "\n",
        "The scale answers differ from 1-4 to 1-7. They are normalised to a value between 0-1\n",
        "\n",
        "Boolean variables are encoded as 1 or 2. They are reencoded as 0 or 1."
      ],
      "metadata": {
        "id": "fHIdC7i1CqhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_normalised = data_cleaning\n",
        "\n",
        "# Vars that need to be normalized (0-1): v6, v36, v54, v55, v63, v115, v134, v196\n",
        "# Vars that are booleans have value 1 or 2, should be 0 or 1: v9, v51, v57, v58, v59, v60, v61, v93\n",
        "\n",
        "# A subset is created containing all variables which need to be modified\n",
        "normalise_subset = data_normalised[[\"v6\", \"v9\", \"v36\", \"v51\", \"v53\", \"v54\", \"v55\", \"v57\", \"v58\", \"v59\", \"v60\", \"v61\", \"v63\", \"v64\", \"v93\", \"v115\", \"v134\", \"v196\"]]\n",
        "\n",
        "# The subset is normalised\n",
        "normalise_subset = (normalise_subset-normalise_subset.min())/(normalise_subset.max()-normalise_subset.min())\n",
        "\n",
        "# And replaces the original data\n",
        "data_normalised.update(normalise_subset)\n",
        "# data_normalised.reset_index(inplace=True)\n",
        "\n",
        "data_normalised.head(20)"
      ],
      "metadata": {
        "id": "FsqEVf0H3ZYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically convert variable types\n",
        "\n",
        "data_normalised = data_normalised.convert_dtypes()\n",
        "\n",
        "# print(data.dtypes)\n",
        "\n",
        "# Set booleans\n",
        "data_normalised['v9'] = data_normalised['v9'].astype('bool')\n",
        "data_normalised['v51'] = data_normalised['v51'].astype('bool')\n",
        "data_normalised['v57'] = data_normalised['v57'].astype('bool')\n",
        "data_normalised['v58'] = data_normalised['v58'].astype('bool')\n",
        "data_normalised['v59'] = data_normalised['v59'].astype('bool')\n",
        "data_normalised['v60'] = data_normalised['v60'].astype('bool')\n",
        "data_normalised['v61'] = data_normalised['v61'].astype('bool')\n",
        "data_normalised['v93'] = data_normalised['v93'].astype('bool')\n",
        "\n",
        "# Set categorical\n",
        "data_normalised['v52'] = data_normalised['v52'].astype('category')\n",
        "data_normalised['v56'] = data_normalised['v56'].astype('category')\n",
        "data_normalised['v62'] = data_normalised['v62'].astype('category')\n",
        "\n",
        "print(data_normalised.dtypes)"
      ],
      "metadata": {
        "id": "EF4yOLzk5H0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping duplicates\n",
        "\n",
        "During the project, we decided to drop part of the data. However, we later abandoned this when we moved on to use a different approach."
      ],
      "metadata": {
        "id": "ljCCJ3JG6tDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_deduplicated = data_normalised\n",
        "data_deduplicated = data_deduplicated.groupby(data_deduplicated.columns.tolist()).apply(len)\\\n",
        "      .rename('group_count')\\\n",
        "      .reset_index()\n",
        "#      .sort_values(['group_count'], ascending = False)\n",
        "data_deduplicated"
      ],
      "metadata": {
        "id": "0EnIff-h6gia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the resulting group counts\n",
        "data_deduplicated['group_count'].unique()\n"
      ],
      "metadata": {
        "id": "leHNVEok7C1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking all columns are included\n",
        "data_deduplicated.columns.tolist()"
      ],
      "metadata": {
        "id": "fWwVPsir7KdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if type settings are still in order.\n",
        "print(data_deduplicated.dtypes)"
      ],
      "metadata": {
        "id": "g3LcKJcl7rIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clearing RAM\n",
        "\n",
        "To delay running out of RAM, we drop some data"
      ],
      "metadata": {
        "id": "Mn2oqsE_LcTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = 0\n",
        "data_filtered = 0\n",
        "data_cleaning = 0"
      ],
      "metadata": {
        "id": "EX6-nB_1Li9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "eIGufCcy7eVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clustered = data_normalised\n",
        "\n",
        "# Empty data_normalised\n",
        "data_normalised = 0\n"
      ],
      "metadata": {
        "id": "EPxqxWJz-wAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA (For fun and to get familiar with clustering)"
      ],
      "metadata": {
        "id": "Nx6Ia5G8E8b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components='mle', svd_solver='full')\n",
        "pca.fit(data_clustered)\n",
        "PCA(n_components=2, svd_solver='full')\n",
        "print(pca.explained_variance_ratio_)\n",
        "# print(pca.singular_values_)"
      ],
      "metadata": {
        "id": "QKFhdwDIE7rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying t-SNE to reduce dataset to 2D"
      ],
      "metadata": {
        "id": "MyuUq7Jj-ylv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#m = TSNE(learning_rate=10)\n",
        "m = TSNE(learning_rate=1500, random_state=234, perplexity=125, verbose=1)\n",
        "\n",
        "# To save time, we can limit the amount of rows to use when experimenting.\n",
        "# data_clustered = data_clustered.sample(n=250).drop(['group_count'], axis=1)\n",
        "# data_clustered.head\n",
        "\n",
        "tsne_features = m.fit_transform(data_clustered)\n",
        "tsne_features[1:4,:]"
      ],
      "metadata": {
        "id": "MVCG2Wxr712z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the resulting t-SNE feature as columns to the original dataset.\n",
        "data_clustered['x'] = tsne_features[:,0]\n",
        "data_clustered['y'] = tsne_features[:,1]"
      ],
      "metadata": {
        "id": "fHxDin5a9G1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the t-SNE points using religion as colour encoding\n",
        "sns.scatterplot(x=\"x\", y=\"y\", hue=\"v52\", data=data_clustered)"
      ],
      "metadata": {
        "id": "9iSgjUhj9bsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coords = data_clustered[['y', 'x']].to_numpy() \n",
        "score = silhouette_score(coords, data_clustered['v52'], metric='euclidean')\n",
        "print('Silhouette Score based on religion: %.3f' % score)\n"
      ],
      "metadata": {
        "id": "ulxZsNL8Fgz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying K-Means on t-SNE data"
      ],
      "metadata": {
        "id": "WHCf1odY-1-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_subset_km = data_clustered[['x', 'y']]\n",
        "\n",
        "sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(data_subset_km)\n",
        "    sum_of_squared_distances.append(km.inertia_)\n",
        "\n",
        "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ksttMPLh9yWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=4)\n",
        "\n",
        "km.fit(data_subset_km)\n",
        "\n",
        "data_subset_km['cl'] = km.labels_\n",
        "data_subset_km.plot.scatter('x', 'y', c='cl', colormap='gist_rainbow')"
      ],
      "metadata": {
        "id": "XiVcNfZ6998A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating distance matrix"
      ],
      "metadata": {
        "id": "j503JeDYAfC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dist = DistanceMetric.get_metric('euclidean')\n",
        "\n",
        "two_d = data_clustered[['x', 'y']].to_numpy()\n",
        "\n",
        "distance = dist.pairwise(data_clustered[['x', 'y']].to_numpy())\n",
        "\n",
        "# This command will often crash the runtime environment because of lack of RAM when not using a reduced data set\n",
        "#dm = distance_matrix(two_d_array['x'], two_d_array['y'])"
      ],
      "metadata": {
        "id": "XDNCRTNG-ria"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the Euclidean distance for each possible pair of nodes. The input for this code are the 2D coordinates of the dataset that was reduced in dimensionality with t-SNE. This data must be converted to a 2D array with the .to_numpy() function."
      ],
      "metadata": {
        "id": "ZZLIWBYkCOpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code will make the Google Colab runtime crash by using all available RAM when not using a reduced data set\n",
        "\n",
        "# pairwise = pd.DataFrame(\n",
        "#     squareform(pdist(data_clustered[['x', 'y']].to_numpy())),\n",
        "#     columns = data_clustered[['x', 'y']].index,\n",
        "#     index = data_clustered[['x', 'y']].index\n",
        "# )"
      ],
      "metadata": {
        "id": "vG2YeD10A4Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpMAP requires us to remove edges with low similarity, therefore:\n",
        "If distance < 10, set to NaN"
      ],
      "metadata": {
        "id": "gVs5qU9mGiYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairwise.applymap(lambda x: np.nan if x > 10 else x)"
      ],
      "metadata": {
        "id": "mgn2w0A2GVrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the distance matrix"
      ],
      "metadata": {
        "id": "9rGkW4ePAy1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.from_pandas_adjacency(pairwise)\n",
        "G = nx.relabel_nodes(G, dict(zip(range(len(G.nodes())),string.ascii_uppercase)))    \n",
        "\n",
        "G = nx.drawing.nx_agraph.to_agraph(G)\n",
        "\n",
        "G.node_attr.update(color=\"red\", style=\"filled\")\n",
        "G.edge_attr.update(color=\"blue\", width=\"0.1\")\n",
        "\n",
        "G.draw('/tmp/out.png', format='png', prog='neato')\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "# smaller nodes and fonts\n",
        "plt.figure(2)\n",
        "nx.draw(G,pos,node_size=60,font_size=8) \n",
        "# larger figure size\n",
        "plt.figure(3,figsize=(12,12)) \n",
        "nx.draw(G,pos)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GiBwq13sCWl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the edges to json, so they can be read by OpMAP"
      ],
      "metadata": {
        "id": "QQHQD9diGqhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairwiseJSON = pairwise.to_json()"
      ],
      "metadata": {
        "id": "03JU7jedGo8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('personal.json', 'w') as json_file:\n",
        "    json.dump(pairwiseJSON, json_file)"
      ],
      "metadata": {
        "id": "PfeCJNqqGwCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN to reduce the number of nodes\n",
        "\n",
        "The code in this section is adapted from <https://arxiv.org/abs/1803.08101>"
      ],
      "metadata": {
        "id": "kbiMrwQzHEzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# command to display matplotlib plots inline within the ipython notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "eFKMSrqMHILI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dbscan = data_clustered[['x', 'y']]\n",
        "\n",
        "clustering = DBSCAN(eps=5, min_samples=150).fit(dataset_dbscan)"
      ],
      "metadata": {
        "id": "jmyxfKb2OkuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=\"x\", y=\"y\", hue=clustering.labels_, palette=\"tab10\", data=dataset_dbscan)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
      ],
      "metadata": {
        "id": "5Mf1sAdsOh08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coords = dataset_dbscan\n",
        "score = silhouette_score(coords, clustering.labels_, metric='euclidean')\n",
        "print('Silhouette Score based on religion: %.3f' % score)"
      ],
      "metadata": {
        "id": "ow3Rm4exOwAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_centermost_point(cluster):\n",
        "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
        "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
        "    return tuple(centermost_point)"
      ],
      "metadata": {
        "id": "ina38I51HMlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dbscan_reduce(df, epsilon, x='x', y='y'):\n",
        "    start_time = time.time()\n",
        "    coords = df[[y, x]].to_numpy()    \n",
        "    db = DBSCAN(eps=epsilon, min_samples=1, algorithm='auto', metric='euclidean').fit(coords)\n",
        "    cluster_labels = db.labels_\n",
        "    num_clusters = len(set(cluster_labels))\n",
        "    print('\\nEpsilon: {:,}'.format(epsilon))\n",
        "    print('Number of clusters: {:,}'.format(num_clusters))\n",
        "    \n",
        "    clusters = pd.Series([coords[cluster_labels==n] for n in range(num_clusters)])\n",
        "    \n",
        "    # find the point in each cluster that is closest to its centroid\n",
        "    centermost_points = clusters.map(get_centermost_point)\n",
        "\n",
        "    # unzip the list of centermost points (lat, lon) tuples into separate lat and lon lists\n",
        "    lats, lons = zip(*centermost_points)\n",
        "    rep_points = pd.DataFrame({x:lons, y:lats})\n",
        "    rep_points.tail()\n",
        "    \n",
        "    # pull row from original data set where lat/lon match the lat/lon of each row of representative points\n",
        "    rs = rep_points.apply(lambda row: df[(df[y]==row[y]) & (df[x]==row[x])].iloc[0], axis=1)\n",
        "    \n",
        "    # all done, print outcome\n",
        "    message = 'Clustered {:,} points down to {:,} points, for {:.2f}% compression in {:,.2f} seconds.'\n",
        "    print(message.format(len(df), len(rs), 100*(1 - float(len(rs)) / len(df)), time.time()-start_time))\n",
        "\n",
        "    score = silhouette_score(coords, db.labels_, metric='euclidean')\n",
        "    print('Silhouette Score: %.3f' % score)\n",
        "\n",
        "    return rs"
      ],
      "metadata": {
        "id": "2poJ4FSaHOcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first cluster the full data set\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.25)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.4)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.5)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.55)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.6)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.65)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.7)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.71)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.72)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.73)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.74)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.75)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.8)\n",
        "data_reduced = dbscan_reduce(data_clustered, epsilon=0.81)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.82)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.83)\n",
        "\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.85)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=0.9)\n",
        "\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=1)\n",
        "# data_reduced = dbscan_reduce(data_clustered, epsilon=2)\n"
      ],
      "metadata": {
        "id": "Ru2IBKmvHRJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_reduced"
      ],
      "metadata": {
        "id": "sRA8f95PHvPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=\"x\", y=\"y\", hue=\"v52\", data=data_reduced)"
      ],
      "metadata": {
        "id": "QmPb7gRTHyzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next, sample every nth row (where n=sample_rate) of the full data set\n",
        "sample_rate = 20\n",
        "df_sampled = data_clustered.iloc[range(0, len(data_clustered), sample_rate)]\n",
        "len(df_sampled)"
      ],
      "metadata": {
        "id": "Y7J2ogr4H4W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine the clustered and sampled sets\n",
        "df_combined = pd.concat([data_clustered, df_sampled], axis=0)\n",
        "df_final = df_combined.reset_index().drop(labels='index', axis=1)"
      ],
      "metadata": {
        "id": "Vjhf_DRmIYtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a map of the worldwide data points\n",
        "fig, ax = plt.subplots(figsize=[11, 8])\n",
        "#rs_scatter = ax.scatter(df_final['x'], df_final['y'], c='m', edgecolor='None', alpha=0.3, s=120)\n",
        "# rs_scatter = ax.scatter(data_clustered['x'], data_clustered['y'], c='m', edgecolor='None', alpha=0.3, s=120)\n",
        "rs_scatter = ax.scatter(data_clustered['x'], data_clustered['y'], c=data_clustered['v52'], edgecolor='None', alpha=0.3, s=120)\n",
        "\n",
        "df_scatter = ax.scatter(data_clustered['x'], data_clustered['y'], c='k', alpha=0.5, s=3)\n",
        "ax.set_title('Full data set vs DBSCAN reduced set')\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "#ax.legend([df_scatter, rs_scatter], ['Full set', 'Reduced set'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ddj5zdjbIdSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a map of the worldwide data points\n",
        "fig2, ax = plt.subplots(figsize=[11, 8])\n",
        "#rs_scatter = ax.scatter(df_final['x'], df_final['y'], c='m', edgecolor='None', alpha=0.3, s=120)\n",
        "# rs_scatter = ax.scatter(data_clustered['x'], data_clustered['y'], c='m', edgecolor='None', alpha=0.3, s=120)\n",
        "rs_scatter = ax.scatter(data_clustered['x'], data_clustered['y'], c=data_clustered['v52'], edgecolor='None', alpha=0.3, s=120)\n",
        "\n",
        "ax.set_title('DBSCAN reduced set')\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "#ax.legend([df_scatter, rs_scatter], ['Full set', 'Reduced set'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oSgILTIzIvVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter Tuning for eps-value\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# The larger the data set, the larger the value of MinPts should be.\n",
        "# If the data set is noisier, choose a larger value of MinPts.\n",
        "# Generally, MinPts should be greater than or equal to the dimensionality of the data set.\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "nbrs = neigh.fit(data_subset)\n",
        "distances, indices = nbrs.kneighbors(data_subset)"
      ],
      "metadata": {
        "id": "6CFcTLBXI1KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The optimal value for epsilon will be found at the point of maximum curvature.\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "plt.plot(distances)"
      ],
      "metadata": {
        "id": "rHoUQo4UI49_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a map of the worldwide data points\n",
        "fig, ax = plt.subplots(figsize=[11, 8])\n",
        "rs_scatter = ax.scatter(data_clustered['x'], data_clustered['y'], c='m', edgecolor='None', alpha=0.3, s=120)\n",
        "# df_scatter = ax.scatter(data_clustered['x'], data_clustered['y'], c='k', alpha=0.5, s=3)\n",
        "ax.set_title('Full data set vs DBSCAN reduced set')\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "ax.legend([df_scatter, rs_scatter], ['Full set', 'Reduced set'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Utsh6kL_I-67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means optimal number of clusters on full data set"
      ],
      "metadata": {
        "id": "5_-xuRlaJGeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_km = data_clustered.drop(['x', 'y', 'id'], axis=1)\n",
        "\n",
        "sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(df_km)\n",
        "    sum_of_squared_distances.append(km.inertia_)\n",
        "\n",
        "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2wXkxW2sJJ0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = sk.cluster.KMeans(n_clusters=6)\n",
        "\n",
        "km.fit(df_km)\n",
        "\n",
        "df_km['cl'] = km.labels_\n",
        "\n",
        "df_km['cl']\n",
        "# df_km.plot.scatter('x', 'y', c='cl', colormap='gist_rainbow')"
      ],
      "metadata": {
        "id": "6eJWsO9xP4Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting the clustered data for D3.js"
      ],
      "metadata": {
        "id": "fKrLnr6VaevJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clustered['id'] = np.arange(len(data_clustered))\n",
        "\n",
        "data_clustered.head(5)\n",
        "\n",
        "export_test = data_clustered.drop(['x', 'y'], axis=1).to_json(orient=\"records\")\n",
        "\n",
        "export_test"
      ],
      "metadata": {
        "id": "iTycbg_bP53P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('node_info.json', 'w') as f:\n",
        "    json.dump(parsed, f)"
      ],
      "metadata": {
        "id": "FvQRg36aQImx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairwise_dbscan = pd.DataFrame(\n",
        "    squareform(pdist(data_clustered[['x', 'y']].to_numpy())),\n",
        "    columns = data_clustered[['x', 'y']].index,\n",
        "    index = data_clustered[['x', 'y']].index\n",
        ")"
      ],
      "metadata": {
        "id": "OAh2zzqcQLSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairwise_dbscan_clean = pairwise_dbscan.applymap(lambda x: np.nan if x > 5 or x==0 else x)\n",
        "\n",
        "pairwise_dbscan_clean"
      ],
      "metadata": {
        "id": "vfE3l1XOQOWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale values to [0, 10] but inverse (so distance becomes weight. High distance = low weight and vice versa)\n",
        "\n",
        "pairwise_dbscan_clean_inv = pairwise_dbscan_clean.applymap(lambda x: 5 - x)\n",
        "\n",
        "pairwise_dbscan_clean_inv"
      ],
      "metadata": {
        "id": "SdsRMoI0QQCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import string\n",
        "# import pygraphviz\n",
        "\n",
        "G = nx.from_pandas_adjacency(pairwise_dbscan_clean_inv)\n",
        "\n",
        "# G = nx.relabel_nodes(G, dict(zip(range(len(G.nodes())),string.ascii_uppercase)))    \n",
        "\n",
        "# G = nx.drawing.nx_agraph.to_agraph(G)\n",
        "\n",
        "# G.node_attr.update(color=\"red\", style=\"filled\")\n",
        "# G.edge_attr.update(color=\"blue\", width=\"0.1\")\n",
        "\n",
        "# G.draw('/tmp/out.png', format='png', prog='neato')\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "# smaller nodes and fonts\n",
        "plt.figure(2)\n",
        "nx.draw(G,pos,node_size=60,font_size=8) \n",
        "# larger figure size\n",
        "plt.figure(3,figsize=(12,12)) \n",
        "nx.draw(G,pos)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f4KDbz2bQSlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from networkx.readwrite import json_graph\n",
        "\n",
        "data2 = json_graph.node_link_data(\n",
        "    G, {\"link\": \"edges\", \"source\": \"source\", \"target\": \"target\", \"weight\": \"value\"}\n",
        ")"
      ],
      "metadata": {
        "id": "iwO4skZTQT9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nodes.json', 'w') as fp:\n",
        "    fp.write(\n",
        "        '[' +\n",
        "        ',\\n'.join(json.dumps(i) for i in data2['nodes']) +\n",
        "        ']\\n')"
      ],
      "metadata": {
        "id": "W15SI-fEQWQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2['nodes']"
      ],
      "metadata": {
        "id": "BItsvnwbQXgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('edges.json', 'w') as fp:\n",
        "    fp.write(\n",
        "        '[' +\n",
        "        ',\\n'.join(json.dumps(i) for i in data2['edges']) +\n",
        "        ']\\n')"
      ],
      "metadata": {
        "id": "uRoxiK8PQZDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agglomerative clustering"
      ],
      "metadata": {
        "id": "kJgx4OaDCip5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustering = AgglomerativeClustering(affinity='precomputed', linkage='complete', n_clusters=6)\n",
        "\n",
        "p = clustering.fit_predict(pairwise)\n",
        "\n",
        "p.shape\n",
        "# AgglomerativeClustering()\n",
        "# clustering.labels_\n",
        "# array([1, 1, 1, 0, 0, 0])"
      ],
      "metadata": {
        "id": "en8-p24NCfJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering.labels_"
      ],
      "metadata": {
        "id": "hn_o7GBpCyU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dist(point_1:tuple, point_2:tuple):\n",
        "    return math.sqrt((point_1[0]-point_2[0])**2 + (point_1[1]-point_2[1])**2)\n",
        "\n",
        "def process(input_list, threshold=10):   \n",
        "    combos = itertools.combinations(input_list, 2)\n",
        "    points_to_remove = [point2 for point1, point2 in combos if get_dist(point1, point2)<=threshold]\n",
        "    p_t_r = np.vstack(points_to_remove)\n",
        "    # print(len(p_t_r))\n",
        "    # print(len(input_list))\n",
        "    # print(input_list)\n",
        "\n",
        "    points_to_keep = [point for point in input_list if point not in p_t_r]\n",
        "\n",
        "    return points_to_keep\n",
        "\n",
        "# coords = [[12,24], [5, 12],[100,1020], [20,30], [121,214], [15,12]]\n",
        "\n",
        "# print(itertools.combinations(data_subset[['x', 'y']][1:250].to_numpy(), 1))\n",
        "\n",
        "# ptr = process(coords)\n",
        "\n",
        "ptk = process(data_subset[['x', 'y']].to_numpy())\n",
        "\n",
        "ptk_clean = np.vstack(ptr_2)\n",
        "\n",
        "print(ptk_clean)\n",
        "\n",
        "# print(data_subset[['x', 'y']].to_numpy()[52])\n",
        "\n",
        "# print(coords[0])\n",
        "# len(ptr)\n",
        "\n",
        "# >>> [[12, 24], [100, 1020], [121, 214]]\n"
      ],
      "metadata": {
        "id": "EK4rJGMYDBud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}